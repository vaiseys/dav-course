---
title: "MD05 Demo"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

Load packages and set theme.

```{r}
#| message: false
library(tidyverse)
library(moderndive)
library(patchwork)
theme_set(theme_light())
```

Let's look at the MD evaluations data.

```{r}
data(evals)
glimpse(evals)
```

We are going to ask whether the "beauty" of an instructor predicts their teaching evaluations.

```{r}
d <- evals |>
  rename(bty = bty_avg,    # just shorter to type
         sex = gender)     # actually what they have

glimpse(d)
```

Let's look at the first few cases.

```{r}
head(d)
```

We can use the **skimr** package for summary statistics if we want. There are lots of ways to do this.

```{r}
library(skimr)
skim(d)
```

You could also just use `summary()`. You don't get the cool little histograms, though.

```{r}
summary(d)
```

It's always good practice to look at the distributions of each variable.

```{r}
ggplot(d,
       aes(x = score)) +
  geom_histogram(boundary = 4,
                 binwidth = .25,
                 color = "white")

ggplot(d,
       aes(x = bty)) +
  geom_histogram(boundary = 4,
                 binwidth = .5,
                 color = "white") +
  scale_x_continuous(breaks = 2:9)
```

Let's look at the data, using `geom_jitter()` to avoid overplotting.

```{r}
ggplot(d,
       aes(x = bty,
           y = score)) +
  geom_jitter(alpha = .3)
```

It's hard to tell how strongly (if at all) these two variables are correlated. We can do this a few different ways.

```{r}
d |> get_correlation(score ~ bty)     # MD wrapper function (tibble)
d |> select(score, bty) |> cor()      # base R version (matrix)
```

This correlation isn't super high, but it's not nothing. Let's recall what the correlation means. It's the expected difference in one variable when the other variable is one standard deviation higher.

Linear regression is another way to summarize a bivariate (i.e., two-way) relationship between quantitative variables. It's useful because later we can extend to more dimensions of predictors.

We are going to introduce two new functions here:

-   `lm()`: a base R function that estimates a linear regression
-   `get_regression_table()`: a **moderndive** function that is a wrapper on `broom::tidy()`. You are welcome to use `broom::tidy()` or something else if you'd like.

```{r}
mod1 <- lm(score ~ bty,
           data = d)

get_regression_table(mod1)
```

For now, please ignore every column to the right of `estimate`. We'll talk about that later. For now let's consider the estimates. What do these mean?

Begin by looking at this plot.

```{r}
ggplot(d,
       aes(x = bty,
           y = score)) +
  geom_jitter(alpha = .3) +
  geom_smooth(method = "lm",     # does this look familiar?
              se = FALSE)        # don't want to use "standard errors" for now
```

Let's look at a slightly different version that makes the x-axis go all the way down to zero.

```{r, echo=FALSE}
ggplot(d,
       aes(x = bty,
           y = score)) +
  geom_jitter(alpha = .3) +
  geom_smooth(method = "lm",
              se = FALSE,
              fullrange = TRUE) +
  scale_x_continuous(limits = c(0,8.5)) +
  geom_vline(xintercept = 0,
             color = "red",
             linetype = "dotted")
```

Now we have a clear picture of what these numbers mean. The **intercept** (often called $\alpha$ or "alpha") is the expected value of $Y$ when $X = 0$. The **slope** (often called $\beta$ or "beta") is the expected difference in $Y$ when $X$ is one unit higher.

If we wanted, we could make our `geom_smooth()` manually once we know the intercept and slope.

```{r}
p <- ggplot(d,
       aes(x = bty,
           y = score)) +
  geom_jitter(alpha = .3)

p + geom_abline(intercept = 3.88,
                slope = .067,
                color = "blue",
                size = 1.5)
```

This should look familiar from middle school. You learned the formula for a line is:

$$
y = mx + b
$$

In statistics, it's a bit different:

$$
\hat{y} = \alpha + \beta x
$$

Why $\hat{y}$ (pronounced "y hat")? Because as you can see, the line doesn't pass through all the points. It's not a *deterministic function* like a line in geometry. It's an *expectation* that *summarizes* the relationship between two variables with a single line that is the best possible fit.

So if the intercept is 3.88 and the slope is .067, what is the expected evaluation score for an instructor with a "beauty" rating of 8? What about a "beauty" rating of 2?

## One categorical predictor

Let's look at sex differences. Can you tell if there's a difference between the groups?

```{r}
ggplot(d,
       aes(x = score,
           y = sex)) +
  geom_jitter(alpha = .3,
              height = .2)
```

Superimpose a boxplot.

```{r}
ggplot(d,
       aes(x = score,
           y = sex)) +
  geom_boxplot(coef = 0,
               outlier.alpha = 0,
               width = .5) +
  geom_jitter(alpha = .3,
              height = .2) 
```

Do a regression with one categorical predictor.

```{r}
mod2 <- lm(score ~ sex,
           data = d) 

broom::tidy(mod2)     # instead of get_regression_table()
```

## Now you practice

Use the `evals` data to test some ideas of your own. Call the file like this:

```         
2023-10-10-inclass.qmd
```

Commit it to your repo by the end of class.

## Units

\<mtcars demo\>

## What *is* regression?

Let's start with the idea of a normal distribution.

```{r}
set.seed(12345)
# fake data
fd <- tibble(x1 = rnorm(n = 500,
                        mean = 500,
                        sd = 100),
             x2 = rnorm(n = 500,
                        mean = 500,
                        sd = 50))
# wider SD
p1 <- ggplot(fd,
             aes(x = x1)) +
  geom_histogram(color = "white",
                 boundary = 500,
                 binwidth = 25) +
  scale_x_continuous(limits = c(200,800))

# narrower SD
p2 <- ggplot(fd,
             aes(x = x2)) +
  geom_histogram(color = "white",
                 boundary = 500,
                 binwidth = 25) +
  scale_x_continuous(limits = c(200,800))

# put together
p1 / p2

```

Formula for a normal distribution.

$$
y_i \sim \text{Normal}(\mu, \sigma)
$$

Formula for a linear regression.

$$
\begin{aligned}y_i &\sim \text{Normal}(\mu_i, \sigma) \\\mu_i &= \alpha + \beta x_i\end{aligned}
$$

Now any value of $X$ will have its own **conditional mean**.
